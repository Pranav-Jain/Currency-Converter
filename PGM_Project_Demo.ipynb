{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGM_Project_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pranav-Jain/Currency-Converter/blob/master/PGM_Project_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KQpL2YBAzo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import statements\n",
        "from IPython.display import Image, display, HTML\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "! pip install unidecode\n",
        "! pip install gdown\n",
        "! mkdir trained_models\n",
        "!mkdir helper\n",
        "!mkdir imgs\n",
        "import gdown\n",
        "# url = 'https://drive.google.com/uc?export=download&id=1QFemk0hz7ZXj4TbQMBCEUzW5PXebCryJ'\n",
        "url = 'https://drive.google.com/uc?export=download&id=1xTzseH_MCrxzTkGoxB9YgvcnUtpLROpr'\n",
        "output = 'trained_models/modelb_checkpoint.pt'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGXhhrbym64-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir output_dir\n",
        "glow_file = 'helper/glow.py'\n",
        "glow_url = 'https://drive.google.com/uc?export=download&id=13SzQ9XkriORBMlUlCZ3JGbm7rwG5CRmB'\n",
        "gdown.download(glow_url,glow_file,quiet=False)\n",
        "\n",
        "celeba_file = 'helper/celeba.py'\n",
        "celeba_url = 'https://drive.google.com/uc?export=download&id=1SjC2DKob0inUXHqyT5bd0lWMH-Akxm2p'\n",
        "gdown.download(celeba_url,celeba_file,quiet=False)\n",
        "\n",
        "# output_dir = 'output_dir/zs.pt'\n",
        "# output_url = 'https://drive.google.com/uc?export=download&id=1WnOoSxyorzywVNtNPonb1TY0Ml3IhwWJ'\n",
        "# gdown.download(output_url,output_dir,quiet=False)\n",
        "\n",
        "output_dir = 'output_dir/z_manipulate.pt'\n",
        "output_url = 'https://drive.google.com/uc?export=download&id=19wXodJ_z228rMtzyrawHt41H_gbJXlM8'\n",
        "gdown.download(output_url,output_dir,quiet=False)\n",
        "\n",
        "# output_dir = 'output_dir/attrs.pt'\n",
        "# output_url = 'https://drive.google.com/uc?export=download&id=1BDhpUeQwwQ9SV65qtcQyj0OjxF8u6w1X'\n",
        "# gdown.download(output_url,output_dir,quiet=False)\n",
        "\n",
        "img_dir = 'imgs/000174.jpg'\n",
        "img_url = 'https://drive.google.com/uc?export=download&id=1HouTgDb0n72AIk7qJwESwRNcYZtPCsnm'\n",
        "gdown.download(img_url,img_dir,quiet=False)\n",
        "\n",
        "img_dir = 'imgs/000214.jpg'\n",
        "img_url = 'https://drive.google.com/uc?export=download&id='\n",
        "gdown.download(img_url,img_dir,quiet=False)\n",
        "\n",
        "img_dir = 'imgs/000382.jpg'\n",
        "img_url = 'https://drive.google.com/uc?export=download&id='\n",
        "gdown.download(img_url,img_dir,quiet=False)\n",
        "\n",
        "img_dir = 'imgs/002844.jpg'\n",
        "img_url = 'https://drive.google.com/uc?export=download&id='\n",
        "gdown.download(img_url,img_dir,quiet=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o32egODOsUwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from PIL import Image\n",
        "import sys\n",
        "sys.path.append('/content/helper/')\n",
        "from torchvision.datasets import MNIST\n",
        "from celeba import CelebA\n",
        "from glow import Glow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8DZaFJ_pGDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_dataset(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    zs = []\n",
        "    attrs = []\n",
        "    for i, (x,y) in enumerate(dataloader):\n",
        "        print('Encoding [{}/{}]'.format(i+1, len(dataloader)), end='\\r')\n",
        "        x = x.to(args['device'])\n",
        "        zs_i, _ = model(x)\n",
        "        zs.append(torch.cat([z.flatten(1) for z in zs_i], dim=1))\n",
        "        attrs.append(y)\n",
        "\n",
        "    zs = torch.cat(zs, dim=0)\n",
        "    attrs = torch.cat(attrs, dim=0)\n",
        "    print('Encoding completed.')\n",
        "    return zs, attrs\n",
        "\n",
        "def compute_dz(zs, attrs, idx):\n",
        "    \"\"\" for a given attribute idx, compute the mean for all encoded z's corresponding to the positive and negative attribute \"\"\"\n",
        "    z_pos = [zs[i] for i in range(len(zs)) if attrs[i][idx] == +1]\n",
        "    z_neg = [zs[i] for i in range(len(zs)) if attrs[i][idx] == -1]\n",
        "    # dz = z_pos - z_neg; where z_pos is mean of all encoded datapoints where attr is present;\n",
        "    return torch.stack(z_pos).mean(0) - torch.stack(z_neg).mean(0)   # out tensor of shape (flattened zs dim,)\n",
        "\n",
        "def get_manipulators(zs, attrs):\n",
        "    \"\"\" compute dz (= z_pos - z_neg) for each attribute \"\"\"\n",
        "    print('Extracting manipulators...', end=' ')\n",
        "    dzs = 1.6 * torch.stack([compute_dz(zs, attrs, i) for i in range(attrs.shape[1])], dim=0)  # compute dz for each attribute official code multiplies by 1.6 scalar here\n",
        "    print('Completed.')\n",
        "    return dzs  # out (n_attributes, flattened zs dim)\n",
        "    \n",
        "def manipulate(model, z, dz, z_std, alpha):\n",
        "    # 1. record incoming shapes\n",
        "    z_dims   = [z_.squeeze().shape   for z_ in z]\n",
        "    z_numels = [z_.numel() for z_ in z]\n",
        "    # 2. flatten z into a vector and manipulate by alpha in the direction of dz\n",
        "    z = torch.cat([z_.flatten(1) for z_ in z], dim=1).to(dz.device)\n",
        "    # print(dz,z)\n",
        "    z = z + dz * torch.tensor(alpha).float().view(-1,1).to(dz.device)  # out (n_alphas, flattened zs dim)\n",
        "    # 3. reshape back to z shapes from each level of the model\n",
        "    zs = [z_.view((len(alpha), *dim)) for z_, dim in zip(z.split(z_numels, dim=1), z_dims)]\n",
        "    # 4. decode\n",
        "    return model.inverse(zs, z_std=z_std)[0]\n",
        "\n",
        "def load_manipulators(model, args):\n",
        "    # construct dataloader with limited number of images\n",
        "    args['mini_data_size'] = 30000\n",
        "    # load z manipulators for each attribute\n",
        "    if os.path.exists(os.path.join(args['output_dir'], 'z_manipulate.pt')):\n",
        "        z_manipulate = torch.load(os.path.join(args['output_dir'], 'z_manipulate.pt'), map_location=args['device'])\n",
        "    else:\n",
        "        # encode dataset, compute manipulators, store zs, attributes, and dzs\n",
        "        dataloader = fetch_dataloader(args, train=True)\n",
        "        zs, attrs = encode_dataset(model, dataloader)\n",
        "        z_manipulate = get_manipulators(zs, attrs)\n",
        "        torch.save(zs, os.path.join(args['output_dir'], 'zs.pt'))\n",
        "        torch.save(attrs, os.path.join(args['output_dir'], 'attrs.pt'))\n",
        "        torch.save(z_manipulate, os.path.join(args['output_dir'], 'z_manipulate.pt'))\n",
        "    return z_manipulate\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize(model, args,attrs=None, alphas=None, img_path=None, n_examples=1):\n",
        "    \"\"\" manipulate an input image along a given attribute \"\"\"\n",
        "    # dataset = fetch_dataloader(args, train=False).dataset  # pull the dataset to access transforms and attrs\n",
        "    # if no attrs passed, manipulate all of them\n",
        "    z_std=0.6\n",
        "    if not attrs:\n",
        "        attrs = list(range(len(dataset.attr_names)))\n",
        "        # print('Attrs',attrs.item())\n",
        "    # if image is passed, manipulate only the image\n",
        "    if img_path is not None:\n",
        "        img = img_path\n",
        "        x = img\n",
        "        # img = Image.open(img_path)\n",
        "        # print('Img size',img.size)\n",
        "        # x = dataset.transform(img)  # transform image to tensor and encode\n",
        "    else:  # take first n_examples from the dataset\n",
        "        x, _ = dataset[0]\n",
        "    z, _ = model(x.unsqueeze(0).to(args['device']))\n",
        "    # get manipulors\n",
        "    z_manipulate = load_manipulators(model,args)\n",
        "    # decode the varied attributes\n",
        "    dec_x =[]\n",
        "    for attr_idx in attrs:\n",
        "        dec_x.append(manipulate(model, z, z_manipulate[attr_idx].unsqueeze(0), z_std, alphas))\n",
        "    return torch.stack(dec_x).cpu()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsoZdjKxabdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {}\n",
        "args['input_dims'] = (3,64,64)\n",
        "args['dataset'] = 'celeba'\n",
        "args['distributed']= False\n",
        "args['train'] = False\n",
        "args['batch_size_init'] = 256\n",
        "args['batch_size'] = 40\n",
        "args['device'] = torch.device('cuda')\n",
        "args['width'] = 512\n",
        "args['depth'] = 32\n",
        "args['n_levels'] = 3\n",
        "args['data_dir'] = '/content/drive/My Drive/PGM Project/datasets/data/'\n",
        "args['output_dir'] = '/content/output_dir/'\n",
        "args['mini_data_size'] = None\n",
        "args['n_bits'] = 5\n",
        "args['vis_alphas'] = [-3,-2,-1,0,1,2,3]\n",
        "# os.makedirs(args['output_dir'],exist_ok=True)\n",
        "model = Glow(args['width'], args['depth'], args['n_levels'],args['input_dims'],False).to(args['device'])\n",
        "model = torch.nn.parallel.DataParallel(model)\n",
        "model.base_dist = model.module.base_dist\n",
        "model.log_prob = model.module.log_prob\n",
        "model.inverse = model.module.inverse\n",
        "\n",
        "model_checkpoint = torch.load(output, map_location=args['device'])\n",
        "model.load_state_dict(model_checkpoint['state_dict'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GwdBUkVQOKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_images(im1, im2):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    columns = 2\n",
        "    im1,im2 = pil_transform(im1),pil_transform(im2)\n",
        "    images = [im1,im2]\n",
        "    # images = [mpimg.imread(im1), mpimg.imread(im2)]\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "        plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENHELNdI9EQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "my_imgs = glob.glob('/content/imgs/*.jpg')\n",
        "print(my_imgs)\n",
        "filename = my_imgs[2]\n",
        "attrs = [28,39,23,9,24]\n",
        "flag=0\n",
        "base_params = [0,0,0,0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2eKDkgwn27k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_results(cur_img,changed_param_idx,new_value):\n",
        "    dec_x = visualize(model,args,attrs=[attrs[changed_param_idx]],alphas=args['vis_alphas'] ,img_path = cur_img)\n",
        "    print(dec_x.shape)\n",
        "    param_idx_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
        "    param_idx = param_idx_map[new_value]\n",
        "    im2 = dec_x[0,param_idx,:,:,:]\n",
        "    im2 = im2.view(*args['input_dims'])\n",
        "    # im2 = pil_transform(im2)\n",
        "    return im2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViwNwuGOz0TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_params = [0,0,0,0,0]\n",
        "cur_params = base_params\n",
        "\n",
        "trnsfm = T.Compose([\n",
        "                    T.Resize((64,64)),\n",
        "                    T.Lambda(lambda im: np.array(im, dtype=np.float32)),                     # to numpy\n",
        "                    T.Lambda(lambda x: np.floor(x / 2**(8 - args['n_bits'])) / 2**args['n_bits']), # lower bits\n",
        "                    T.ToTensor(),  # note: if input to this transform is uint8, it divides by 255 and returns float\n",
        "                    T.Lambda(lambda t: t + torch.rand_like(t) / 2**args['n_bits']\n",
        "                )])   \n",
        "# dataset = fetch_dataloader(args, train=False).dataset \n",
        "# print(dataset)\n",
        "changed_param_idx = 0\n",
        "new_value = 0\n",
        "\n",
        "pil_transform = T.ToPILImage()\n",
        "\n",
        "im1 = Image.open(filename)\n",
        "im1 = trnsfm(im1)\n",
        "# im1 = pil_transform(im1)\n",
        "base_img = im1\n",
        "print(base_img.size)\n",
        "cur_img = base_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWERP69lHccB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Inputs { display-mode: \"form\", run: \"auto\"}\n",
        "Hairline = 0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "__Age___ = 0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "__Eyes__ = 0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "_Blonde_ = 0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "_Beard__ = 0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "params = [Hairline, __Age___, __Eyes__, _Blonde_ , _Beard__]\n",
        "print(params)\n",
        "print(cur_params)\n",
        "changed_param_idx = int(np.nonzero(np.array(cur_params)-np.array(params))[0])\n",
        "new_value = params[changed_param_idx]\n",
        "\n",
        "im2 = parse_results(cur_img,changed_param_idx,new_value)\n",
        "show_images(base_img,im2)\n",
        "print(changed_param_idx)\n",
        "cur_params = params\n",
        "# Image(filename)\n",
        "\n",
        "# show_images(filename,filename)\n",
        "#insert code here to generate image using params\n",
        "# to show original image and updated image, run \"show_images(filename, <updated file name>)\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}